{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d947b8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:lightgreen;font-family:newtimeroman;font-size:22px;line-height:1.7em;text-align:center;border-radius:5px 5px\">Temporal Demand Forecasting for Taxi Services_ A Multi_Model_Comparison</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4459a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\envs\\machinelearning\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\envs\\machinelearning\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\envs\\machinelearning\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\venka\\anaconda3\\envs\\machinelearning\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "619/619 [==============================] - 1s 799us/step\n",
      "619/619 [==============================] - 1s 980us/step\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.svm import SVR\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "class TaxiDemandDataProcessor:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.merged_df = self.load_data()\n",
    "        self.preprocess_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        dfs = []\n",
    "\n",
    "        # Load each JSON file into a separate DataFrame\n",
    "        for json_file in os.listdir(self.folder_path):\n",
    "            if json_file.endswith('.json'):\n",
    "                file_path = os.path.join(self.folder_path, json_file)\n",
    "                df = pd.read_json(file_path)\n",
    "                dfs.append(df)\n",
    "\n",
    "        # Merge DataFrames into a single DataFrame\n",
    "        merged_df = pd.concat(dfs, ignore_index=True)\n",
    "        return merged_df\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Convert time-related columns to datetime\n",
    "        self.merged_df['startTime'] = pd.to_datetime(self.merged_df['startTime'])\n",
    "        self.merged_df['endTime'] = pd.to_datetime(self.merged_df['endTime'])\n",
    "\n",
    "        # Round the start time down to the nearest 15-minute interval\n",
    "        self.merged_df['startInterval'] = self.merged_df['startTime'].dt.floor('15min')\n",
    "\n",
    "        # Round the end time up to the nearest 15-minute interval\n",
    "        self.merged_df['endInterval'] = self.merged_df['endTime'].dt.ceil('15min')\n",
    "\n",
    "        # Extract relevant features\n",
    "        self.merged_df['hour_of_day'] = self.merged_df['startTime'].dt.hour\n",
    "        self.merged_df['day_of_week'] = self.merged_df['startTime'].dt.dayofweek\n",
    "\n",
    "        # Calculate the duration of each trip in minutes\n",
    "        self.merged_df['tripDurationMinutes'] = (\n",
    "            (self.merged_df['endInterval'] - self.merged_df['startInterval']).dt.total_seconds() / 60\n",
    "        ).astype(float)\n",
    "\n",
    "        # Add a demand column for each 15-minute interval\n",
    "        demand_per_interval = self.merged_df.groupby('startInterval').size().reset_index(name='demand')\n",
    "\n",
    "        # Merge the demand back to the original dataframe\n",
    "        self.merged_df = self.merged_df.merge(demand_per_interval, on='startInterval', how='left')\n",
    "\n",
    "    def split_data(self):\n",
    "        # Split data into training and testing sets\n",
    "        features = ['tripDurationMinutes', 'hour_of_day', 'day_of_week']\n",
    "        X = self.merged_df[features]\n",
    "        y = self.merged_df['demand']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train_linear_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Train a linear regression model\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        reg_model = LinearRegression()\n",
    "        reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        linear_predictions = reg_model.predict(X_test_scaled)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, linear_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, linear_predictions))\n",
    "        r2 = r2_score(y_test, linear_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def train_xgb_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Train an XGBoost model\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n",
    "        r2 = r2_score(y_test, xgb_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def train_arima_model(self, y_train, y_test):\n",
    "        # Train an ARIMA model\n",
    "        arima_model = ARIMA(y_train, order=(5, 1, 0))\n",
    "        arima_results = arima_model.fit()\n",
    "\n",
    "        # Make predictions\n",
    "        arima_predictions = arima_results.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, typ='levels')\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, arima_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, arima_predictions))\n",
    "        r2 = r2_score(y_test, arima_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def train_dnn_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Train a Deep Neural Network (DNN) model\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Make predictions\n",
    "        dnn_predictions = model.predict(X_test).flatten()\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, dnn_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, dnn_predictions))\n",
    "        r2 = r2_score(y_test, dnn_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def train_random_forest_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Train a Random Forest model\n",
    "        rf_model = RandomForestRegressor()\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, rf_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
    "        r2 = r2_score(y_test, rf_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def train_svr_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Train a Support Vector Regression (SVR) model\n",
    "        svr_model = SVR()\n",
    "        svr_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        svr_predictions = svr_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, svr_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, svr_predictions))\n",
    "        r2 = r2_score(y_test, svr_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "    def reshape_for_lstm(self, X):\n",
    "        # Reshape the input data for LSTM\n",
    "        return X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "    def train_lstm_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Reshape input data for LSTM\n",
    "        X_train_lstm = self.reshape_for_lstm(X_train)\n",
    "        X_test_lstm = self.reshape_for_lstm(X_test)\n",
    "\n",
    "        # Build LSTM model\n",
    "        lstm_model = Sequential()\n",
    "        lstm_model.add(LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "        lstm_model.add(Dense(1))\n",
    "        lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        # Train the model\n",
    "        lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Make predictions\n",
    "        lstm_predictions = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "        # Reshape predictions to 1D array\n",
    "        lstm_predictions = lstm_predictions.reshape(lstm_predictions.shape[0])\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, lstm_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, lstm_predictions))\n",
    "        r2 = r2_score(y_test, lstm_predictions)\n",
    "        adj_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)\n",
    "\n",
    "        return {'MAE': mae, 'RMSE': rmse, 'R-squared': r2, 'Adjusted R-squared': adj_r2}\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"C:\\\\Users\\\\venka\\\\Downloads\\\\Meiro_Mobility_Assessment_Jan2022Jul2023\"\n",
    "taxi_demand_processor = TaxiDemandDataProcessor(folder_path)\n",
    "X_train, X_test, y_train, y_test = taxi_demand_processor.split_data()\n",
    "\n",
    "# Train and evaluate models\n",
    "linear_metrics = taxi_demand_processor.train_linear_model(X_train, X_test, y_train, y_test)\n",
    "xgb_metrics = taxi_demand_processor.train_xgb_model(X_train, X_test, y_train, y_test)\n",
    "arima_metrics = taxi_demand_processor.train_arima_model(y_train, y_test)\n",
    "dnn_metrics = taxi_demand_processor.train_dnn_model(X_train, X_test, y_train, y_test)\n",
    "rf_metrics = taxi_demand_processor.train_random_forest_model(X_train, X_test, y_train, y_test)\n",
    "svr_metrics = taxi_demand_processor.train_svr_model(X_train, X_test, y_train, y_test)\n",
    "lstm_metrics = taxi_demand_processor.train_lstm_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        'Model': ['Linear Regression', 'XGBoost', 'ARIMA', 'DNN', 'Random Forest', 'SVR', 'LSTM'],\n",
    "        'MAE': [linear_metrics['MAE'], xgb_metrics['MAE'], arima_metrics['MAE'], dnn_metrics['MAE'], rf_metrics['MAE'], svr_metrics['MAE'], lstm_metrics['MAE']],\n",
    "        'RMSE': [linear_metrics['RMSE'], xgb_metrics['RMSE'], arima_metrics['RMSE'], dnn_metrics['RMSE'], rf_metrics['RMSE'], svr_metrics['RMSE'], lstm_metrics['RMSE']],\n",
    "        'R-squared': [linear_metrics['R-squared'], xgb_metrics['R-squared'], arima_metrics['R-squared'], dnn_metrics['R-squared'], rf_metrics['R-squared'], svr_metrics['R-squared'], lstm_metrics['R-squared']],\n",
    "        'Adjusted R-squared': [linear_metrics['Adjusted R-squared'], xgb_metrics['Adjusted R-squared'], arima_metrics['Adjusted R-squared'], dnn_metrics['Adjusted R-squared'], rf_metrics['Adjusted R-squared'], svr_metrics['Adjusted R-squared'], lstm_metrics['Adjusted R-squared']],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c8a3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R-squared</th>\n",
       "      <th>Adjusted R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>7.597536</td>\n",
       "      <td>10.994138</td>\n",
       "      <td>0.183756</td>\n",
       "      <td>0.183632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>5.269429</td>\n",
       "      <td>8.897213</td>\n",
       "      <td>0.465429</td>\n",
       "      <td>0.465348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARIMA</td>\n",
       "      <td>10.216816</td>\n",
       "      <td>12.744965</td>\n",
       "      <td>-0.096920</td>\n",
       "      <td>-0.096920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DNN</td>\n",
       "      <td>5.784177</td>\n",
       "      <td>9.331762</td>\n",
       "      <td>0.411935</td>\n",
       "      <td>0.411846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>5.269207</td>\n",
       "      <td>8.897078</td>\n",
       "      <td>0.465445</td>\n",
       "      <td>0.465364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVR</td>\n",
       "      <td>5.883358</td>\n",
       "      <td>10.454428</td>\n",
       "      <td>0.261929</td>\n",
       "      <td>0.261817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>6.650818</td>\n",
       "      <td>9.844253</td>\n",
       "      <td>0.345570</td>\n",
       "      <td>0.345471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model        MAE       RMSE  R-squared  Adjusted R-squared\n",
       "0  Linear Regression   7.597536  10.994138   0.183756            0.183632\n",
       "1            XGBoost   5.269429   8.897213   0.465429            0.465348\n",
       "2              ARIMA  10.216816  12.744965  -0.096920           -0.096920\n",
       "3                DNN   5.784177   9.331762   0.411935            0.411846\n",
       "4      Random Forest   5.269207   8.897078   0.465445            0.465364\n",
       "5                SVR   5.883358  10.454428   0.261929            0.261817\n",
       "6               LSTM   6.650818   9.844253   0.345570            0.345471"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55dab11",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue; font-weight:bold;\"><i>Inference :</i></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f464312",
   "metadata": {},
   "source": [
    "The choice of the best model depends on the specific requirements of your use case and the importance you place on each metric. Let's analyze the metrics:\n",
    "\n",
    "#### I) Analyzation of metrics:\n",
    "\n",
    "1 MAE (Mean Absolute Error):\n",
    "\n",
    "* Lower values are better.\n",
    "\n",
    "* Random Forest has the lowest MAE, followed closely by XGBoost.\n",
    "\n",
    "2 RMSE (Root Mean Squared Error):\n",
    "\n",
    "* Lower values are better.\n",
    "\n",
    "* Random Forest and XGBoost have the lowest RMSE.\n",
    "\n",
    "3 R-squared:\n",
    "\n",
    "* Higher values are better, indicating better explanatory power.\n",
    "\n",
    "* XGBoost has the highest R-squared, followed by Random Forest and DNN.\n",
    "\n",
    "4 Adjusted R-squared:\n",
    "\n",
    "* Similar to R-squared but adjusted for the number of predictors.\n",
    "\n",
    "* XGBoost and Random Forest have the highest Adjusted R-squared.\n",
    "\n",
    "#### II) Recommendations:\n",
    "\n",
    "1 XGBoost and Random Forest:\n",
    "\n",
    "* Both XGBoost and Random Forest consistently perform well across all metrics.\n",
    "\n",
    "* Consider using either XGBoost or Random Forest based on your preferences and interpretability.\n",
    "\n",
    "2 DNN (Deep Neural Network):\n",
    "\n",
    "* DNN also performs well, particularly in terms of MAE and RMSE.\n",
    "\n",
    "* If interpretability is less critical and computational resources are available, DNN could be a good choice.\n",
    "\n",
    "3 LSTM:\n",
    "\n",
    "* LSTM has reasonable performance, but it may not outperform XGBoost or Random Forest in this specific use case.\n",
    "\n",
    "4 Linear Regression and ARIMA:\n",
    "\n",
    "* Linear Regression and ARIMA appear to have lower performance compared to ensemble methods (XGBoost, Random Forest) and DNN.\n",
    "\n",
    "In conclusion, based on the provided metrics, XGBoost and Random Forest stand out as strong candidates for the use case.\n",
    "\n",
    "Consider further evaluation, such as cross-validation, hyperparameter tuning, and possibly an ensemble approach, to make a final decision based on your specific requirements and the characteristics of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376eb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
